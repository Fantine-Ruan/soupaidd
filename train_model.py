import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import joblib
import os

print("ğŸ¤– å¼€å§‹è®­ç»ƒæ±¤å“é¢„æµ‹AI...")

# ===== ç¬¬1æ­¥ï¼šåŠ è½½æ¸…æ´—å¥½çš„æ•°æ® =====
current_folder = os.path.dirname(os.path.abspath(__file__))
history_df = pd.read_csv(os.path.join(current_folder, 'history_cleaned.csv'), encoding='utf-8')

print(f"ğŸ“Š åŠ è½½æ•°æ®ï¼š{len(history_df)}æ¡å†å²è®°å½•")

# ===== ç¬¬2æ­¥ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆå‘Šè¯‰AIï¼šçœ‹ä»€ä¹ˆç‰¹å¾â†’é¢„æµ‹ä»€ä¹ˆï¼‰=====
print("\nğŸ”§ å‡†å¤‡è®­ç»ƒæ•°æ®...")

# ç‰¹å¾Xï¼ˆAIè¦è§‚å¯Ÿçš„"çº¿ç´¢"ï¼‰ï¼šå¤©æ°”ã€æ¸©åº¦ã€å­£èŠ‚ã€æ˜¯å¦å‘¨æœ«ã€åé¦ˆåˆ†æ•°ç­‰
feature_columns = [
    'æ¸©åº¦', 'å¤©æ°”ç¼–ç ', 'æœˆä»½', 'å­£èŠ‚ç¼–ç ', 'æ˜¯å¦å‘¨æœ«', 'åé¦ˆåˆ†æ•°'
]

# è‡ªåŠ¨æ·»åŠ é£Ÿæç‰¹å¾ï¼ˆæ‰€æœ‰ä»¥"é£Ÿæ_"å¼€å¤´çš„åˆ—ï¼‰
ingredient_cols = [col for col in history_df.columns if col.startswith('é£Ÿæ_')]
feature_columns.extend(ingredient_cols)

print(f"   ä½¿ç”¨ç‰¹å¾ï¼š{len(feature_columns)}ä¸ª")
print(f"   åŒ…æ‹¬ï¼šå¤©æ°”ã€æ¸©åº¦ã€å­£èŠ‚ã€æ˜¯å¦å‘¨æœ«ã€åé¦ˆåˆ†æ•°ã€{len(ingredient_cols)}ç§é£Ÿæ")

# æ„å»ºXï¼ˆç‰¹å¾çŸ©é˜µï¼‰
X = history_df[feature_columns].fillna(0)  # å¦‚æœæœ‰ç©ºå€¼å¡«0

# æ„å»ºyï¼ˆæ ‡ç­¾ï¼šè¦é¢„æµ‹çš„ç›®æ ‡â€”â€”æ±¤åï¼‰
y = history_df['æ±¤å']

# æŠŠæ±¤åè½¬æˆæ•°å­—ï¼ˆAIåªè®¤è¯†æ•°å­—ï¼Œä¸è®¤è¯†ä¸­æ–‡ï¼‰
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

print(f"   é¢„æµ‹ç›®æ ‡ï¼š{len(label_encoder.classes_)}ç§æ±¤")
print(f"   åŒ…æ‹¬ï¼š{', '.join(label_encoder.classes_)}")

# ===== ç¬¬3æ­¥ï¼šåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† =====
# å› ä¸ºæ•°æ®å°‘ï¼Œæˆ‘ä»¬ç”¨80%è®­ç»ƒï¼Œ20%æµ‹è¯•ï¼ˆå¦‚æœåªæœ‰10æ¡ï¼Œå°±8æ¡è®­ç»ƒï¼Œ2æ¡æµ‹è¯•ï¼‰
if len(history_df) >= 15:  # æé«˜åˆ°15æ¡æ‰åšæµ‹è¯•
    try:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
        )
        print(f"   è®­ç»ƒé›†ï¼š{len(X_train)}æ¡ï¼Œæµ‹è¯•é›†ï¼š{len(X_test)}æ¡")
    except ValueError as e:
        # å¦‚æœæŸç§æ±¤åªå‡ºç°1æ¬¡ï¼Œåˆ†å±‚æŠ½æ ·ä¼šå¤±è´¥ï¼Œæ”¹æˆéšæœºåˆ’åˆ†
        print(f"   æ³¨æ„ï¼šæŸäº›æ±¤å“è®°å½•å¤ªå°‘ï¼ˆåªå–è¿‡1æ¬¡ï¼‰ï¼Œæ— æ³•åˆ†å±‚æµ‹è¯•")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=0.2, random_state=42  # å»æ‰ stratify
        )
        print(f"   æ”¹ç”¨éšæœºåˆ’åˆ†ï¼šè®­ç»ƒé›†{len(X_train)}æ¡ï¼Œæµ‹è¯•é›†{len(X_test)}æ¡")
else:
    # æ•°æ®å¤ªå°‘ï¼Œå…¨éƒ¨è®­ç»ƒï¼Œä¸æµ‹å‡†ç¡®ç‡ï¼ˆåæ­£å­¦äº†æ€»æ¯”ä¸å­¦å¥½ï¼‰
    X_train, y_train = X, y_encoded
    X_test, y_test = None, None
    print(f"   æ•°æ®è¾ƒå°‘ï¼ˆ{len(history_df)}æ¡ï¼‰ï¼Œå…¨éƒ¨ç”¨äºå­¦ä¹ ï¼ˆæš‚ä¸æµ‹è¯•å‡†ç¡®ç‡ï¼‰")

# ===== ç¬¬4æ­¥ï¼šè®­ç»ƒæ¨¡å‹ï¼ˆæ ¸å¿ƒï¼ï¼‰=====
print("\nğŸ¯ å¼€å§‹è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")

# åˆ›å»ºæ¨¡å‹ï¼ˆéšæœºæ£®æ—ï¼šç”±100æ£µå†³ç­–æ ‘æŠ•ç¥¨å†³å®šï¼Œä¸å®¹æ˜“é”™ï¼‰
model = RandomForestClassifier(
    n_estimators=100,      # 100æ£µæ ‘æŠ•ç¥¨
    max_depth=5,           # æ ‘ä¸è¦å¤ªæ·±ï¼ˆé˜²æ­¢æ­»è®°ç¡¬èƒŒï¼‰
    min_samples_split=2,   # æœ€å°‘2ä¸ªæ ·æœ¬æ‰åˆ†å‰
    random_state=42,       # å›ºå®šéšæœºç§å­ï¼ˆæ¯æ¬¡ç»“æœä¸€æ ·ï¼‰
    class_weight='balanced' # å¦‚æœæŸæ±¤å‡ºç°å°‘ï¼Œä¹Ÿå…¬å¹³å¯¹å¾…
)

# å¼€å§‹è®­ç»ƒï¼ˆæ‹Ÿåˆï¼‰
model.fit(X_train, y_train)
print("   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

# ===== ç¬¬5æ­¥ï¼šè¯„ä¼°æ¨¡å‹ï¼ˆçœ‹çœ‹å­¦å¾—æ€ä¹ˆæ ·ï¼‰=====
if X_test is not None:
    print("\nğŸ“ˆ æ¨¡å‹è¯„ä¼°ï¼š")
    
    # é¢„æµ‹æµ‹è¯•é›†
    y_pred = model.predict(X_test)
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = accuracy_score(y_test, y_pred)
    print(f"   é¢„æµ‹å‡†ç¡®ç‡ï¼š{accuracy*100:.1f}%")
    
    # è¯¦ç»†æŠ¥å‘Šï¼ˆçœ‹æ¯ç§æ±¤é¢„æµ‹å¾—å‡†ä¸å‡†ï¼‰
    print("\n   è¯¦ç»†æŠ¥å‘Šï¼š")
    target_names = label_encoder.inverse_transform(np.unique(y_test))
    print(classification_report(y_test, y_pred, target_names=target_names))
else:
    print("\nâš ï¸  æ•°æ®é‡å°ï¼Œè·³è¿‡è¯„ä¼°ï¼ˆå»ºè®®ç§¯ç´¯20æ¡ä»¥ä¸Šæ•°æ®å†è¯„ä¼°ï¼‰")

# ===== ç¬¬6æ­¥ï¼šçœ‹çœ‹AIæœ€çœ‹é‡ä»€ä¹ˆç‰¹å¾ï¼ˆå¯è§£é‡Šæ€§ï¼‰=====
print("\nğŸ” AIå†³ç­–ä¾æ®ï¼ˆç‰¹å¾é‡è¦æ€§ï¼‰ï¼š")

# è·å–ç‰¹å¾é‡è¦æ€§
importances = model.feature_importances_
feature_importance_df = pd.DataFrame({
    'ç‰¹å¾': feature_columns,
    'é‡è¦æ€§': importances
}).sort_values('é‡è¦æ€§', ascending=False)

# æ˜¾ç¤ºå‰5ä¸ªé‡è¦ç‰¹å¾
print("   æœ€é‡è¦çš„5ä¸ªå› ç´ ï¼š")
for idx, row in feature_importance_df.head(5).iterrows():
    print(f"   {row['ç‰¹å¾']}: {row['é‡è¦æ€§']*100:.1f}%")

# ===== ç¬¬7æ­¥ï¼šä¿å­˜æ¨¡å‹ï¼ˆä¸‹æ¬¡ç›´æ¥åŠ è½½ç”¨ï¼‰=====
print("\nğŸ’¾ ä¿å­˜æ¨¡å‹æ–‡ä»¶...")

# ä¿å­˜æ¨¡å‹
model_path = os.path.join(current_folder, 'soup_predictor_model.pkl')
joblib.dump(model, model_path)

# ä¿å­˜æ ‡ç­¾ç¼–ç å™¨ï¼ˆæŠŠæ•°å­—å˜å›æ±¤åç”¨ï¼‰
encoder_path = os.path.join(current_folder, 'label_encoder.pkl')
joblib.dump(label_encoder, encoder_path)

# ä¿å­˜ç‰¹å¾åˆ—è¡¨ï¼ˆé¢„æµ‹æ—¶è¦çŸ¥é“æœ‰å“ªäº›ç‰¹å¾ï¼‰
feature_path = os.path.join(current_folder, 'feature_columns.pkl')
joblib.dump(feature_columns, feature_path)

print(f"   âœ… æ¨¡å‹å·²ä¿å­˜ï¼šsoup_predictor_model.pkl")
print(f"   âœ… æ ‡ç­¾æ˜ å°„å·²ä¿å­˜ï¼šlabel_encoder.pkl")
print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼ä½ çš„AIå·²ç»å­¦ä¼šäº†{len(label_encoder.classes_)}ç§æ±¤çš„é…æ–¹ï¼")

# ===== å½©è›‹ï¼šæµ‹è¯•é¢„æµ‹æ˜å¤© =====
print("\nğŸ”® åšä¸ªå°æµ‹è¯•ï¼šå‡è®¾æ˜å¤©æ˜¯æƒ…å†µï¼ŒAIä¼šæ¨èä»€ä¹ˆï¼Ÿ")

# å‡è®¾æ˜å¤©ï¼šå‘¨å…­ï¼Œæ™´å¤©ï¼Œ20åº¦ï¼Œæ˜¥å¤©ï¼Œæœ‰æ’éª¨å’Œç‰ç±³
tomorrow = pd.DataFrame([{
    'æ¸©åº¦': 20,
    'å¤©æ°”ç¼–ç ': 3,  # æ™´
    'æœˆä»½': 3,
    'å­£èŠ‚ç¼–ç ': 1,  # æ˜¥
    'æ˜¯å¦å‘¨æœ«': 1,  # æ˜¯å‘¨æœ«
    'åé¦ˆåˆ†æ•°': 80
}])

# è¡¥é½é£Ÿæåˆ—ï¼ˆæ²¡æœ‰çš„å°±å¡«0ï¼‰
for col in ingredient_cols:
    if col not in tomorrow.columns:
        tomorrow[col] = 0

# å‡è®¾å†°ç®±é‡Œæœ‰æ’éª¨å’Œç‰ç±³ï¼ˆè®¾ä¸º1ï¼‰
if 'é£Ÿæ_æ’éª¨' in tomorrow.columns:
    tomorrow['é£Ÿæ_æ’éª¨'] = 1
if 'é£Ÿæ_ç‰ç±³' in tomorrow.columns:
    tomorrow['é£Ÿæ_ç‰ç±³'] = 1
